{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from keras import Sequential\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(r'..\\..\\data\\train\\data-train-lstm.csv')\n",
    "data_test = pd.read_csv(r'..\\..\\data\\test\\data-test-lstm.csv')\n",
    "\n",
    "data_train.abstract = data_train.abstract.astype(str)\n",
    "data_train.journal = data_train.journal.astype(str)\n",
    "data_test.abstract = data_test.abstract.astype(str)\n",
    "data_test.journal = data_test.journal.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = data_train.sample(frac=0.2)\n",
    "# data_test = data_test.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train['abstract']\n",
    "y_train = data_train['journal']\n",
    "x_test = data_test['abstract']\n",
    "y_test = data_test['journal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construcción de vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(df):\n",
    "    text_split = []\n",
    "\n",
    "    for text in df:\n",
    "        for s in text.split():\n",
    "            text_split.append(s)\n",
    "\n",
    "    return text_split\n",
    "\n",
    "docs = split_words(x_train)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 400\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390053"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding de artículos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = pad_sequences(tokenizer.texts_to_sequences(x_train), padding=\"post\", maxlen=MAXLEN)\n",
    "x_test_padded = pad_sequences(tokenizer.texts_to_sequences(x_test), padding=\"post\", maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146385, 400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignación de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PubMed downloaded from http://evexdb.org/pmresources/vec-space-models/\n",
    "f = open('PubMed-w2v.txt')\n",
    "cabecera = f.readline()\n",
    "EMBED_SIZE =  int(cabecera.split()[1])\n",
    "print(EMBED_SIZE)\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_SIZE))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > VOCAB_SIZE - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = Word2Vec(vector_size=EMBED_SIZE, workers=-1) #son muchos 1000 dimensiones\n",
    "# word2vec.build_vocab(docs)\n",
    "# word2vec.train(docs, total_examples=len(docs), epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBED_SIZE = 300\n",
    "# embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_SIZE))\n",
    "\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     if word in word2vec.wv:\n",
    "#         embedding_matrix[i] = word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexación de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = set(y_train.unique())\n",
    "class_to_index = dict((c,i) for i, c in enumerate(classes))\n",
    "# index_to_class = dict((v,k) for k, v in class_to_index.items())\n",
    "\n",
    "names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])\n",
    "train_labels = names_to_ids(y_train)\n",
    "test_labels = names_to_ids(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construcción y compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import SparseTopKCategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prueba 1\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, trainable=False,\n",
    "#     weights=[embedding_matrix], input_length=MAXLEN),\n",
    "#     LSTM(128, return_sequences=True, dropout=0.2),\n",
    "#     LSTM(64, dropout=0.2),\n",
    "#     Dense(32, activation=\"relu\"),\n",
    "#     Dense(650, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prueba 2\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, trainable=False,\n",
    "#     weights=[embedding_matrix], input_length=MAXLEN),\n",
    "#     Bidirectional(LSTM(128, return_sequences=True, dropout=0.2)),\n",
    "#     Bidirectional(LSTM(64, dropout=0.2)),\n",
    "#     Dense(32, activation=\"relu\"),\n",
    "#     Dense(650, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prueba 3\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, trainable=True,\n",
    "#     weights=[embedding_matrix], input_length=MAXLEN),\n",
    "#     Bidirectional(LSTM(128, return_sequences=True, dropout=0.2)),\n",
    "#     Bidirectional(LSTM(64, dropout=0.2)),\n",
    "#     Dense(650, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba 4\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, trainable=True,\n",
    "    weights=[embedding_matrix], input_length=MAXLEN),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.2)),\n",
    "    Bidirectional(LSTM(64, dropout=0.2)),\n",
    "    Dense(650, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[SparseTopKCategoricalAccuracy(k=10)])\n",
    "\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_padded, train_labels, validation_data=[x_test_padded, test_labels], epochs=1, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test_padded)\n",
    "m = SparseTopKCategoricalAccuracy(k=1)\n",
    "y_true = np.array(test_labels, np.float32).ravel()\n",
    "m.update_state(test_labels,pred)\n",
    "m.result().numpy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be23ae5e9cab8f7d3dc053734d7c1a72c76df2b3e32c536f0ca96d686b8189c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
